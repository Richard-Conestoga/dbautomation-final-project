# Database Automation â€“ Final Project

## âœ… Task 1 â€“ Data Ingestion, Sync & Validation

### ðŸŽ¯ Objective
Build a production-style, automated pipeline that:

- Ingests large NYC 311 CSV data into MySQL using chunked ETL.
- Cleans and normalizes records (dates, boroughs, coordinates).
- Syncs MySQL data incrementally into MongoDB.
- Validates consistency between MySQL and MongoDB.
- Exposes clear telemetry (rows/sec, RAM/CPU) for performance tuning.

---

### ðŸ§  Approach

| Step | Description |
|------|-------------|
| 1ï¸âƒ£ Data Acquisition | Download NYC 311 data (sample + Kaggle 2011/full) via scripts/download_nyc311.py |
| 2ï¸âƒ£ Chunked Ingestion | Load CSV into MySQL in batches using scripts/ingest_mysql.py |
| 3ï¸âƒ£ Data Cleaning | Normalize columns, fix boroughs from ZIP, filter invalid lat/lng |
| 4ï¸âƒ£ Idempotency & Logging | Use ingestion_log table for re-run safety & throughput tracking |
| 5ï¸âƒ£ MongoDB Sync | Incremental, window-based upsert from MySQL â†’ MongoDB via scripts/sync_to_mongo.py |
| 6ï¸âƒ£ Consistency Validation | scripts/validate_consistency.py compares MySQL vs MongoDB counts |

---

### ðŸ§¹ Data Cleaning Highlights

- Drop rows with missing `unique_key` or `created_date`.
- Normalize column names (snake_case) and parse dates with explicit format.
- Infer missing `borough` from `incident_zip` (ZIP prefix â†’ borough map).
- Enforce NYC coordinate bounds (lat 40.5â€“40.9, lng âˆ’74.3 to âˆ’73.7).
- Drop duplicates on `unique_key` (keep latest).

These steps are implemented in `scripts/ingest_mysql.py` (`clean_chunk` function) and documented as a â€œcleaning checklistâ€ in the code comments.

---

### ðŸ“ˆ Telemetry & Idempotency

**MySQL ETL (scripts/ingest_mysql.py):**

- Reads CSV with `chunksize=BATCH_SIZE` (default 10,000).
- Logs per-chunk:
  - rows ingested
  - rows/sec
  - RAM (MB) via `psutil`
  - CPU%
- Wraps batch inserts in `try/except` + `rollback` for transactional safety.
- Uses `ON DUPLICATE KEY UPDATE` on `unique_key` for idempotent upserts.
- Writes to an `ingestion_log` table with:
  - `dataset_file`
  - `ingested_rows`
  - `elapsed_seconds`
  - `rows_per_sec`
  - `loaded_at`

**MongoDB Sync (scripts/sync_to_mongo.py):**

- Parses the year from `NYC311_CSV` filename â†’ defines window `[year-01-01, (year+1)-01-01)`.
- Preserves MySQL as source of truth; cleans only previous Mongo docs in that window.
- Fetches rows for window and upserts in MongoDB using `bulk_write` + `UpdateOne({...}, {"$set": doc}, upsert=True)`.
- Logs per-batch:
  - ops in batch
  - new vs updated docs
  - ops/sec
- Stores sync metadata in `mongo_sync_log` collection (window, status, rows_synced, duration).

---

### ðŸ” Automated Validation

After sync, `scripts/validate_consistency.py` and `sync_to_mongo.py`:

- Compare `COUNT(*)` in MySQL vs `count_documents()` in MongoDB for the same date window.
- Print a warning if counts differ.
- CI pipeline fails if one side is empty or counts mismatch, enforcing data consistency.

---

### ðŸ’¡ Performance Notes

- **Batch size (10,000 rows)** chosen to balance:
  - Good throughput (~6â€“7k rows/sec on 2011 dataset locally).
  - Controlled RAM usage (~130â€“150 MB).
  - Reasonable rollback scope on failure.
- CI uses a **25k-row synthetic sample** (`nyc_311_2023_sample.csv`) for fast runs; locally we also test with the **1.2 GB 2011 Kaggle file** for realistic performance.

---

## ðŸ¤– Task 3 â€“ Anomaly Detection & Optimization

### ðŸ“Œ Objective
Build an anomaly detection module using Python (Pandas + Scikit-learn) to identify abnormal NYC311 requests, store flagged rows in a separate table, analyze the results, and suggest performance improvements based on Signoz metrics.

---

### ðŸ§  Approach

| Step | Description |
|------|-------------|
| 1ï¸âƒ£ Data Load | Fetched records from MySQL `service_requests` table using Pandas |
| 2ï¸âƒ£ Anomaly Detection | Detected missing, long-open, and location outlier anomalies |
| 3ï¸âƒ£ Save Results | Stored anomalies into a separate MySQL table called `anomalies` |
| 4ï¸âƒ£ Analysis | Counted anomalies by category and verified with SQL |
| 5ï¸âƒ£ Optimization | Suggested performance improvements using Signoz metrics |

---

### ðŸš¨ Types of Anomalies Detected

| Anomaly Type | Detection Logic |
|--------------|----------------|
| missing_location | Latitude or longitude is NULL |
| long_open_case | Request open > 90 days with no closed_date |
| location_outlier | Out-of-city coordinates identified using IsolationForest ML |

---

### ðŸ“Š Results Summary

| Metric | Value |
|--------|------|
| Total records scanned | 6 |
| Total anomalies detected | 5 |
| Saved into `anomalies` table | âœ”ï¸ Yes |

Screenshots (below) prove the results:
- Record count in service_requests
- Anomalies count
- Breakdown by anomaly_reason
- Sample anomalies
- Terminal script output

(Add screenshots below)

## ðŸ“¸ Screenshots

### 1ï¸âƒ£ Script Execution Output
![script_output](Screenshots/Task3-zafar/script_output.png)

### 2ï¸âƒ£ service_requests Count (6 rows)
![service_requests_count](Screenshots/Task3-zafar/service_requests_count.png)

### 3ï¸âƒ£ anomalies Count (5 anomalies)
![anomalies_count](Screenshots/Task3-zafar/anomalies_count.png)

### 4ï¸âƒ£ Anomaly Category Breakdown
![anomaly_breakdown](Screenshots/Task3-zafar/anomaly_breakdown.png)

### 5ï¸âƒ£ Sample anomaly records from anomalies table
![anomalies_table](Screenshots/Task3-zafar/anomalies_table.png)


### ðŸŽ¯ Why Precision/Recall Not Used
The dataset has **no ground-truth labels**, so accuracy metrics like precision and recall cannot be calculated.  
Instead, anomaly counts by type are analyzed qualitatively.

---

### ðŸš€ SQL Performance Optimization Based on Signoz

| Optimization | Benefit |
|-------------|---------|
| Create index on created_date + borough | Faster lookups without full scans |
| Avoid SELECT * | Less data scanned â†’ faster response |
| Filter recent data only | Prevents scanning entire history â†’ scalable |

Example SQL:
```sql
CREATE INDEX idx_sr_created_borough 
ON service_requests(created_date, borough);

SELECT unique_key, created_date, closed_date, latitude, longitude
FROM service_requests;

SELECT *
FROM service_requests
WHERE created_date >= NOW() - INTERVAL 30 DAY;
